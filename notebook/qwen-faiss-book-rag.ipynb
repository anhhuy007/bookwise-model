{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import faiss\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "import torch\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- Load the Dataset from Hugging Face ---\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatoupines/book-dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m train_data \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()  \u001b[38;5;66;03m# Convert dataset to a Pandas DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# --- Data Cleaning and Formatting ---\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Load the Dataset from Hugging Face ---\n",
    "dataset = load_dataset(\"matoupines/book-dataset\")\n",
    "train_data = dataset[\"train\"].to_pandas()  # Convert dataset to a Pandas DataFrame\n",
    "\n",
    "# --- Data Cleaning and Formatting ---\n",
    "\n",
    "MAX_DESC_LENGTH = 200  # Limit description to 200 characters\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans text and truncates long descriptions.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with single spaces\n",
    "    if len(text) > MAX_DESC_LENGTH:\n",
    "        text = text[:MAX_DESC_LENGTH] + \"...\"  # Truncate long descriptions\n",
    "    return text\n",
    "\n",
    "\n",
    "def format_authors(authors):\n",
    "    \"\"\"Formats the authors field to ensure proper quoting for multiple authors.\"\"\"\n",
    "    if pd.isna(authors):\n",
    "        return \"\"\n",
    "    authors = str(authors)\n",
    "    # If there's a comma, assume multiple authors and enclose in quotes\n",
    "    if \",\" in authors:\n",
    "        return f'\"{authors}\"'\n",
    "    else:\n",
    "        return authors  # Return as is if no comma (single author)\n",
    "\n",
    "\n",
    "# Apply cleaning functions to relevant fields\n",
    "train_data[\"title\"] = train_data[\"title\"].apply(clean_text)\n",
    "train_data[\"description\"] = train_data[\"description\"].apply(clean_text)\n",
    "train_data[\"authors\"] = train_data[\"authors\"].apply(format_authors)\n",
    "\n",
    "# --- Combine Fields for Embedding ---\n",
    "train_data[\"text\"] = (\n",
    "    train_data[\"title\"] + \" \" + train_data[\"authors\"] + \" \" + train_data[\"description\"]\n",
    ")\n",
    "documents = train_data[\"text\"].tolist()\n",
    "\n",
    "# --- Initialize Embedding Model ---\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Generate Embeddings for the Dataset\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = embedding_model.encode(documents, show_progress_bar=True)\n",
    "\n",
    "# Convert embeddings to float32 for FAISS\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# --- Initialize Embedding Model ---\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Generate Embeddings for the Dataset\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = embedding_model.encode(documents, show_progress_bar=True)\n",
    "\n",
    "# Convert embeddings to float32 for FAISS\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# --- Initialize FAISS ---\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Cosine similarity\n",
    "embeddings = embeddings / np.linalg.norm(\n",
    "    embeddings, axis=1, keepdims=True\n",
    ")  # Normalize embeddings\n",
    "index.add(embeddings)  # Add normalized embeddings to FAISS index\n",
    "\n",
    "# Save FAISS index (optional)\n",
    "faiss.write_index(index, \"books_index.faiss\")\n",
    "print(\"FAISS index saved.\")\n",
    "\n",
    "# Load Qwen-2.5 tokenizer and model\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "\n",
    "\n",
    "def truncate_context(query, context, max_input_tokens):\n",
    "    \"\"\"Truncate context to fit within model's token limit.\"\"\"\n",
    "    # Start with smaller context chunks\n",
    "    context_chunks = context.split(\"\\n\")\n",
    "    truncated_chunks = []\n",
    "    current_text = f\"Query: {query}\\nContext:\"\n",
    "\n",
    "    for chunk in context_chunks:\n",
    "        test_text = current_text + f\"\\n{chunk}\\nAnswer:\"\n",
    "        tokens = qwen_tokenizer(\n",
    "            test_text, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens\n",
    "        )\n",
    "\n",
    "        if len(tokens[\"input_ids\"][0]) < max_input_tokens:\n",
    "            truncated_chunks.append(chunk)\n",
    "            current_text = f\"Query: {query}\\nContext: {' '.join(truncated_chunks)}\"\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return current_text + \"\\nAnswer:\"\n",
    "\n",
    "\n",
    "def generate_response(query, context, max_new_tokens=150):\n",
    "    \"\"\"Generates focused responses using context-aware prompting.\"\"\"\n",
    "    max_input_tokens = min(2048, qwen_tokenizer.model_max_length - max_new_tokens)\n",
    "\n",
    "    # Create a more structured prompt\n",
    "    prompt = f\"\"\"You are an expert in book summaries. Based on the following context, answer the query by extracting relevant details from the provided book descriptions. Focus on specific information about the plot, characters, or themes mentioned in the context. \n",
    "    \n",
    "    Query: {query}\n",
    "    Context: {context}\n",
    "    \n",
    "    Detailed Answer:\"\"\"\n",
    "\n",
    "    inputs = qwen_tokenizer(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    outputs = qwen_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=qwen_tokenizer.eos_token_id,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "\n",
    "    return qwen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def get_relevant_docs(query, k=1):\n",
    "    \"\"\"Retrieve relevant documents with semantic search.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(query_embedding.astype(\"float32\"), k)\n",
    "\n",
    "    # Convert to DataFrame and sort by relevance\n",
    "    df = pd.DataFrame(dataset[\"train\"])\n",
    "    results = df.iloc[indices[0]]\n",
    "\n",
    "    # Only return highly relevant results\n",
    "    mask = distances[0] < 1.2  # Adjust threshold as needed\n",
    "    return results[mask], distances[0][mask]\n",
    "\n",
    "\n",
    "try:\n",
    "    query = \"Find me a book about a widow coping with loss and finding new meaning in life.?\"\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs, scores = get_relevant_docs(query)\n",
    "\n",
    "    for score, row in relevant_docs:\n",
    "        if isinstance(row, dict):\n",
    "            print(f\"Title: {row.get('title', 'No title')}\\n\")\n",
    "        elif isinstance(row, str):\n",
    "            print(f\"Content: {row}\\n\")\n",
    "        else:\n",
    "            print(f\"Unknown data type: {type(row)}\\n\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
